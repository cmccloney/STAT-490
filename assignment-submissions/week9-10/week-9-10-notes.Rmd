---
title: "week-9-10-notes"
output:
  html_document: default
  word_document: default
bibliography: week-9-10-notes.bib
---

### _Support Vector Machine â€” Introduction to Machine Learning Algorithms_, @Gandhi

Gandhi writes this article as an introduction to the concept of Support Vector Machines (SVMs). 
SVMs can be used for both regression and classification tasks, and are highly preferred due to 
the high accuracy produced with
relatively less computational power required. The goal of a Support Vector Machine is to define a 
decision boundary, or hyperplane, that distinctly classifies data in N-dimensional space, where N 
is the number of features of the data. 
Data points that influence the position and orientation of the hyperplane are known as 
'support vectors', and are used to determine the margin of the classifier. The width of this 
margin is maximized by minimizing the hinge loss function (another article) plus a regularization 
parameter. A partial derivative with respect to each weight, or each beta coefficient, 
is taken to find the 
gradient of the SVM's loss function. We then update the gradient via the weight based on correct 
or incorrect classification.

### _Support vector machines ( intuitive understanding ) Parts 1,2_, @Kompella

This article serves as an introduction to Support Vector Machines (SVMs), and how their purpose is to linearly seperate data for 
binary classification. SVMs 
look to minimize a loss function, which measures the accuracy of a given SVMs classification. An SVM will place a line to split 
the data, which is known as 
a hyperplane. The orientation and position of this hyperplane is determined by minimizing a loss function, which measures how 
well this hyperplane split the 
data. Examining the loss function, we adjust the weights, which define the hyperplane, and iterate this process with the goal 
of reducing the amount of total error. At the end of all iterations, the total error has been minimized, and the values for the 
weights chosen. A regularization term is added to the hinge loss function, to increase the penalty for 
misclassification of points with high weights. By minimizing the values, we can reduce over-fitting the training data. An 
optimization algorithm is run to 
find the best combination of values of beta coefficients, or weights, available.

### _Large-Scale Support Vector Machines: Algorithms and Theory_, @Menon

Menon (2009) introduces the idea of Large-Scale Learning, which is the idea of doing supervised learning method on a training 
set that is too large to be held in a 
computer's memory. Menon (2009) discusses the theory of  Support Vector Machines (SVMs), how they function. He then discusses 
loss functions and regularization, and the use of gradient 
descent (GD) to minimize the two terms summed. For SVMs, a hinge-loss function is used, with a regularization term added to 
prevent overfitting of training data. Finding the minimum of this function is equivalent to finding the optimum hyperplane 
location and orientation. A 'kernel trick' 
that maps the data into a higher-dimensional space, and then finds a classifier for the data in this space, can be used to 
create a non-linear classifying 
boundary using SVMs. Menon (2009) describes the use of stochastic gradient descent (SGD) in approximating the optimal weight 
vector values, which are used for 
determining the hyperplane, and how it can achieve a lower generalization error than gradient descent in a fixed amount of time. 
GD runtime scales linearly 
with the number of training examples n, whereas SGD is agnostic about size for runtime, yet will also still have lower error as 
n increases. Menon (2009) shows that 
because of this fact, SGD-based algorithms will actually decrease their runtime with a larger set of training data. Although SGD 
can only approximate the 
optimal solution, a good generalization error is still achieved, at much faster relative speed. Menon (2009) discusses a few 
classifying algorithms, but a specific one named Pegasos is based on this SGD design, and is shown to indeed have a runtime that 
decreases as the size of the training data increases. Pegasos works by first subsetting the training data into batches of size 
k. As any k works, and k = 1 is the least 
expensive, this is the default choice. It then iteratively works on these batches, updating the weight vector using a 
subgradient on the objective function, 
which is the error loss function defined above that is to be optimized. It then projects this vector onto a ball of radius of 
$\frac{1}{\sqrt\lambda}$, where $\lambda$ represents the regularization parameter chosen, as Menon (2009) 
proves that the optimal solution lies within this ball, and every projection step brings the algorithm closer to the SVM 
solution. He recommends further 
research into using SVM for regression, and semi-supervised SVMs for partially labelled data, and that using SGD, such as 
Pegasos, can be adapted for these purposes for fast and accurate analysis.

# References
