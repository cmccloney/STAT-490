---
title: "week-9-10-notes"
output:
  html_document: default
  word_document: default
bibliography: week-9-10-notes.bib
---

### _Support Vector Machine â€” Introduction to Machine Learning Algorithms_, @Gandhi

Gandhi (2018) writes this article as an introduction to the concept of Support Vector Machines (SVMs). 
SVMs can be used for both regression and classification tasks, and are highly preferred by many relative to other available methods
due to the high accuracy produced with
relatively less computational power required. The goal of a Support Vector Machine is to define a 
decision boundary, or hyperplane, that distinctly classifies data in p-dimensional space, where p 
is the number of features of the data. 
Data points that influence the position and orientation of the hyperplane are known as 
'support vectors', and are used to determine the margin of the classifier. The width of this 
margin is maximized by minimizing the hinge loss function (another article) plus a regularization 
parameter. A partial derivative with respect to each weight, or each beta coefficient, 
is taken to find the 
gradient of the SVM's loss function. We then update the gradient via the weight based on correct 
or incorrect classification. This gradient update continues until the loss function is minimized,
but the exact stopping criteria details are not provided.

### _Support vector machines ( intuitive understanding ) Parts 1,2_, @Kompella

This article serves as an introduction to Support Vector Machines (SVMs), and how their purpose is to linearly separate data for 
binary classification. SVMs 
look to minimize a loss function, which measures the accuracy of a given SVMs classification. An SVM will place a 
$(p-1)$-dimensional line to split the data, which is known as 
a hyperplane. The orientation and position of this hyperplane is determined by minimizing a loss function, which measures how 
well this hyperplane split the 
data. Examining the loss function, we adjust the weights, which define the hyperplane, and iterate this process with the goal 
of reducing the amount of total error. At the end of all iterations, the total error has been minimized, and the values for the 
weights chosen. A regularization term is added to the hinge loss function, to increase the penalty for 
misclassification of points with high weights. By minimizing the values, we can reduce over-fitting the training data. An 
optimization algorithm is run to 
find the best combination of values of beta coefficients, or weights, available.

### _Large-Scale Support Vector Machines: Algorithms and Theory_, @Menon

Menon (2009) introduces the idea of Large-Scale Learning, which is the idea of doing supervised learning method on a training 
set that is too large to be held in a 
computer's memory. Menon (2009) discusses the theory of  Support Vector Machines (SVMs), how they function. He then discusses 
loss functions and regularization, and the use of gradient 
descent (GD) to minimize the two terms summed. For SVMs, a hinge-loss function is used, with a regularization term added to 
prevent overfitting of training data. Finding the minimum of this function is equivalent to finding the optimum hyperplane 
location and orientation. A 'kernel trick' 
that maps the data into a higher-dimensional space, and then finds a classifier for the data in this space, can be used to 
create a non-linear classifying 
boundary using SVMs. Menon (2009) describes the use of stochastic gradient descent (SGD) in approximating the optimal weight 
vector values, which are used for 
determining the hyperplane, and how it can achieve a lower generalization error than gradient descent in a fixed amount of time. 
GD runtime scales linearly 
with the number of training examples n, whereas SGD is agnostic about size for runtime, yet will also still have lower error as 
n increases. Menon (2009) shows that 
because of this fact, SGD-based algorithms will actually decrease their runtime with a larger set of training data. Although SGD 
can only approximate the 
optimal solution, a good generalization error is still achieved, at much faster relative speed. Menon (2009) discusses a few 
classifying algorithms, but a specific one named Pegasos is based on this SGD design, and is shown to indeed have a runtime that 
decreases as the size of the training data increases. Pegasos works by first subsetting the training data into batches of size 
k. As any k works, and k = 1 is the least 
expensive, this is the default choice. It then iteratively works on these batches, updating the weight vector using a 
subgradient on the objective function, 
which is the error loss function defined above that is to be optimized. It then projects this vector onto a ball of radius of 
$\frac{1}{\sqrt\lambda}$, where $\lambda$ represents the regularization parameter chosen, as Menon (2009) 
proves that the optimal solution lies within this ball, and every projection step brings the algorithm closer to the SVM 
solution. He recommends further 
research into using SVM for regression, and semi-supervised SVMs for partially labelled data, and that using SGD, such as 
Pegasos, can be adapted for these purposes for fast and accurate analysis.

### _Fuzzy Support Vector Machines_, @LinEtAl2002

LinEtAl (2002) begins with explaining the idea of Support Vector Machines (SVMs), and a review of theory behind their operation. They then explain 
that some data points may not be fully assigned to one class, a concept 
called fuzzy membership, or that certain data points should be attributed 
to noise and therefore discarded.Since SVMs lack this ability, LinEtAl (2002)
derive a method called Fuzzy Support Vector Machines (FSVMs) to add in 
this functionality. Let $s_i$ represent fuzzy membership, or the attitude 
of the training point $x_i$ toward one class, and $(1-s_i)$ the attitude 
towards being meaningless, where $0<s_i\leq1$. This allows for some data 
points to be treated as more important than others when attempting to 
determine the orientation and location of a hyperplane. Because $s_i$'s 
are chosen by the researcher, the number of free parameters increases from 
1, the regularization parameter, to $1 + n$, where n is the number of 
training points. These values are chosen by making connections between the 
main property of the dataset and the fuzzy memberships of each point. 
LinEtAl (2002) then go on to conduct experiments using and testing their newly 
designed FSVMs, showing that it can be more accurate classifying a portion 
of the data that is more important, more accurate in regards to only 
one class, or reducing the effect of outliers. The creation of FSVMs 
extends the applications of SVMs to account for data particularities to 
get more specific results, based on the researcher's objectives.

# References
